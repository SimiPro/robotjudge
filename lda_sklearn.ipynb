{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_docs = documents[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(small_docs)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x7358 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 51316 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x7358 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 48 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simi/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "no_topics = 10\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "ocean activity investigated tea cartridges dissolved radical sheep colleague tpa\n",
      "Topic 1:\n",
      "mm blinker bulb atf agenda ban waco lamp starter brightness\n",
      "Topic 2:\n",
      "win event expose british rr text flags columbia receiving mask\n",
      "Topic 3:\n",
      "key keys plane surrender abc effect geb pitt gordon session\n",
      "Topic 4:\n",
      "people just don god like think good know time did\n",
      "Topic 5:\n",
      "israel kk lebanese israeli zone soldiers captain attacks withdraw traded\n",
      "Topic 6:\n",
      "biggest chop offensive defensive tires hit disappointment kings nren nsfnet\n",
      "Topic 7:\n",
      "space section firearm military gm shall government weapon encryption person\n",
      "Topic 8:\n",
      "hole holes exist pointer electron edu metal graphics semiconductor stated\n",
      "Topic 9:\n",
      "edu like use know just com don new think time\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/simi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/simi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "small_docs = documents[:1000]\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = [t.lower_ for t in nlp(text)]\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "    \n",
    "    \n",
    "small_docs = [prepare_text_for_lda(text) for text in small_docs]\n",
    "\n",
    "bigram = gensim.models.Phrases(small_docs)\n",
    "small_docs_bigram = [bigram[t] for t in small_docs]\n",
    "\n",
    "# create dictionary and corpus\n",
    "dictionary = Dictionary(small_docs_bigram)\n",
    "\n",
    "# corpus = (token_id, count_in_curr_doc) , sparse representation\n",
    "corpus = [dictionary.doc2bow(d) for d in small_docs_bigram]\n",
    "small_docs_id = [dictionary.doc2idx(document=tw) for tw in small_docs_bigram]\n",
    "small_docs_id = [d for d in small_docs_id if len(d) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: 20 | V: 14065 | K: 5\n",
      "51283.5390625\n",
      "50248.1171875\n",
      "53486.796875\n",
      "50227.203125\n",
      "50262.421875\n",
      "52337.49609375\n",
      "50557.37109375\n",
      "51631.77734375\n",
      "54041.71875\n",
      "51546.54296875\n",
      "50774.15234375\n",
      "49709.98046875\n",
      "50452.75390625\n",
      "52415.609375\n",
      "49846.59765625\n",
      "51495.2421875\n",
      "49580.69140625\n",
      "49355.63671875\n",
      "52615.34765625\n",
      "50220.3984375\n",
      "49253.5\n",
      "52066.640625\n",
      "51281.6796875\n",
      "51011.921875\n",
      "50575.2578125\n",
      "52674.13671875\n",
      "50046.234375\n",
      "50445.84765625\n",
      "52150.33203125\n",
      "50347.796875\n",
      "50306.72265625\n",
      "51165.25390625\n",
      "50529.5703125\n",
      "48964.56640625\n",
      "49278.4453125\n",
      "52605.9375\n",
      "52989.98828125\n",
      "49878.68359375\n",
      "50004.43359375\n",
      "49499.03125\n",
      "48604.9140625\n",
      "49791.7265625\n",
      "49566.3125\n",
      "49022.3515625\n",
      "49600.484375\n",
      "49437.6796875\n",
      "49043.29296875\n",
      "52324.6171875\n",
      "49675.9921875\n",
      "48750.4140625\n",
      "49885.8125\n",
      "50817.76953125\n",
      "48639.88671875\n",
      "49788.6015625\n",
      "51142.64453125\n",
      "49696.1484375\n",
      "50139.02734375\n",
      "48592.73046875\n",
      "49223.46484375\n",
      "49017.76171875\n",
      "48591.515625\n",
      "50052.1875\n",
      "49721.0078125\n",
      "53080.1328125\n",
      "49243.0625\n",
      "52490.1640625\n",
      "50031.76953125\n",
      "48496.41796875\n",
      "49794.19140625\n",
      "48581.21484375\n",
      "49377.08984375\n",
      "48266.9375\n",
      "49871.87109375\n",
      "50471.4453125\n",
      "50470.7421875\n",
      "49191.34375\n",
      "49172.96875\n",
      "49426.4765625\n",
      "49231.32421875\n",
      "49119.61328125\n",
      "50118.76171875\n",
      "50011.47265625\n",
      "49797.32421875\n",
      "51232.69921875\n",
      "48903.77734375\n",
      "49895.0390625\n",
      "48255.46484375\n",
      "48639.98828125\n",
      "48955.296875\n",
      "49216.3671875\n",
      "48277.43359375\n",
      "51395.640625\n",
      "48559.77734375\n",
      "48094.2109375\n",
      "48799.3359375\n",
      "50208.61328125\n",
      "48874.1484375\n",
      "48611.62890625\n",
      "48526.5703125\n",
      "49643.50390625\n",
      "48286.56640625\n",
      "48170.64453125\n",
      "48356.64453125\n",
      "47936.7109375\n",
      "50723.5390625\n",
      "49225.69921875\n",
      "48028.8203125\n",
      "49813.16015625\n",
      "48885.6953125\n",
      "48143.94921875\n",
      "49044.7109375\n",
      "48242.07421875\n",
      "47727.765625\n",
      "47823.96875\n",
      "48184.24609375\n",
      "48980.0234375\n",
      "48742.88671875\n",
      "47643.0703125\n",
      "48403.46875\n",
      "48706.44140625\n",
      "48159.1953125\n",
      "48572.27734375\n",
      "48174.7421875\n",
      "50176.15234375\n",
      "48452.328125\n",
      "49425.78125\n",
      "47467.24609375\n",
      "47839.61328125\n",
      "48383.40234375\n",
      "47682.69140625\n",
      "49142.56640625\n",
      "47763.16015625\n",
      "49234.8671875\n",
      "48572.55078125\n",
      "47905.70703125\n",
      "48036.04296875\n",
      "47525.87890625\n",
      "48504.74609375\n",
      "51200.70703125\n",
      "47724.19140625\n",
      "48855.1328125\n",
      "48173.3828125\n",
      "48713.1328125\n",
      "48343.37109375\n",
      "48482.125\n",
      "50169.56640625\n",
      "50583.9140625\n",
      "49191.8984375\n",
      "48891.73046875\n",
      "47285.13671875\n",
      "48371.4375\n",
      "48223.265625\n",
      "47187.34765625\n",
      "47545.8046875\n",
      "49373.83984375\n",
      "48360.03125\n",
      "47964.328125\n",
      "47478.58984375\n",
      "47509.5859375\n",
      "47408.3828125\n",
      "48592.40625\n",
      "48167.625\n",
      "49660.47265625\n",
      "48363.69921875\n",
      "47665.73828125\n",
      "47847.61328125\n",
      "49194.2109375\n",
      "48043.19140625\n",
      "48219.5390625\n",
      "47253.73046875\n",
      "50406.08203125\n",
      "49328.8828125\n",
      "47517.80859375\n",
      "48464.07421875\n",
      "47445.7890625\n",
      "47286.015625\n",
      "47951.140625\n",
      "47118.03515625\n",
      "47719.9765625\n",
      "48208.36328125\n",
      "47978.1640625\n",
      "48788.01953125\n",
      "47602.35546875\n",
      "47965.953125\n",
      "47526.05859375\n",
      "47570.16796875\n",
      "47877.41015625\n",
      "48081.56640625\n",
      "47103.15625\n",
      "47263.90625\n",
      "47239.421875\n",
      "48475.7265625\n",
      "47524.97265625\n",
      "47195.12890625\n",
      "47703.98046875\n",
      "47766.53515625\n",
      "47177.4140625\n",
      "48728.03125\n",
      "47436.8515625\n",
      "47641.0390625\n",
      "47453.09765625\n",
      "48450.96875\n",
      "47470.8515625\n",
      "47327.1875\n",
      "47061.609375\n",
      "47338.84375\n",
      "48158.68359375\n",
      "49338.15234375\n",
      "47212.82421875\n",
      "47538.07421875\n",
      "47325.09375\n",
      "48946.4609375\n",
      "47455.87890625\n",
      "47557.2890625\n",
      "47593.40234375\n",
      "48504.96875\n",
      "47221.47265625\n",
      "47376.0859375\n",
      "48028.609375\n",
      "47713.46875\n",
      "47612.89453125\n",
      "47871.015625\n",
      "47151.18359375\n",
      "48601.15234375\n",
      "47399.171875\n",
      "47270.09375\n",
      "46968.96875\n",
      "47428.0390625\n",
      "46900.96484375\n",
      "48637.95703125\n",
      "47037.19921875\n",
      "47518.796875\n",
      "48595.82421875\n",
      "48139.52734375\n",
      "48138.15625\n",
      "47300.14453125\n",
      "47151.8828125\n",
      "48248.51953125\n",
      "47874.8203125\n",
      "47596.81640625\n",
      "47144.09375\n",
      "48447.37890625\n",
      "47584.93359375\n",
      "47080.21875\n",
      "46967.24609375\n",
      "49112.03515625\n",
      "47551.65625\n",
      "47300.4375\n",
      "47576.3828125\n",
      "47621.1171875\n",
      "47777.8515625\n",
      "48274.97265625\n",
      "48533.5390625\n",
      "47542.30078125\n",
      "48358.5390625\n",
      "48236.79296875\n",
      "47041.80859375\n",
      "47549.86328125\n",
      "48369.27734375\n",
      "48105.1875\n",
      "47552.64453125\n",
      "48252.58984375\n",
      "47477.23046875\n",
      "48889.2421875\n",
      "47451.64453125\n",
      "46733.42578125\n",
      "47627.87109375\n",
      "47694.5703125\n",
      "48268.53515625\n",
      "47323.61328125\n",
      "47981.03515625\n",
      "47263.77734375\n",
      "46900.17578125\n",
      "47440.484375\n",
      "46797.3046875\n",
      "48523.33203125\n",
      "47799.3828125\n",
      "47467.3203125\n",
      "47930.98046875\n",
      "46830.7734375\n",
      "47390.87890625\n",
      "47152.328125\n",
      "47610.69921875\n",
      "47122.39453125\n",
      "48116.203125\n",
      "47744.046875\n",
      "47257.4609375\n",
      "47783.1484375\n",
      "46993.59765625\n",
      "48195.875\n",
      "47075.09375\n",
      "47384.359375\n",
      "47514.45703125\n",
      "47042.13671875\n",
      "47141.0\n",
      "49375.1796875\n",
      "47163.8671875\n",
      "47429.7109375\n",
      "47156.15625\n",
      "48063.46875\n",
      "46946.3828125\n",
      "46901.3125\n",
      "47641.46875\n",
      "46917.20703125\n",
      "47266.59765625\n",
      "46686.22265625\n",
      "46715.9453125\n",
      "48061.5390625\n",
      "47330.984375\n",
      "47606.23046875\n",
      "47183.72265625\n",
      "47159.484375\n",
      "47263.7890625\n",
      "47271.37890625\n",
      "47854.359375\n",
      "47655.890625\n",
      "48008.2734375\n",
      "47014.921875\n",
      "47230.73046875\n",
      "46899.38671875\n",
      "46647.9296875\n",
      "47491.2421875\n",
      "46815.4296875\n",
      "46910.328125\n",
      "46879.41796875\n",
      "47366.890625\n",
      "46923.15625\n",
      "47093.984375\n",
      "48039.2578125\n",
      "47982.48046875\n",
      "47290.8125\n",
      "46747.05859375\n",
      "47307.7734375\n",
      "47493.15234375\n",
      "49566.3984375\n",
      "48088.56640625\n",
      "47223.875\n",
      "47062.69140625\n",
      "46882.8203125\n",
      "46834.33984375\n",
      "47641.27734375\n",
      "47709.26953125\n",
      "47943.8828125\n",
      "47486.4609375\n",
      "47054.75\n",
      "47671.73828125\n",
      "47275.91015625\n",
      "47261.79296875\n",
      "46730.30078125\n",
      "47488.421875\n",
      "46752.89453125\n",
      "47032.11328125\n",
      "47252.08203125\n",
      "46796.83203125\n",
      "46735.53125\n",
      "47605.39453125\n",
      "47067.546875\n",
      "47310.10546875\n",
      "47127.2265625\n",
      "46946.87109375\n",
      "47537.83203125\n",
      "47357.109375\n",
      "47051.22265625\n",
      "47053.19921875\n",
      "46730.984375\n",
      "47976.0\n",
      "47548.95703125\n",
      "47889.625\n",
      "46939.80859375\n",
      "47554.9609375\n",
      "47254.03515625\n",
      "47694.48828125\n",
      "46898.6953125\n",
      "47211.359375\n",
      "47259.7578125\n",
      "46966.69140625\n",
      "46758.72265625\n",
      "46965.33203125\n",
      "47149.875\n",
      "47058.41015625\n",
      "47255.67578125\n",
      "46912.06640625\n",
      "46917.1875\n",
      "47002.65234375\n",
      "46763.4609375\n",
      "46808.14453125\n",
      "47970.95703125\n",
      "46868.58984375\n",
      "48199.59375\n",
      "47407.703125\n",
      "47087.5078125\n",
      "48233.41796875\n",
      "46782.66796875\n",
      "47552.8125\n",
      "46609.36328125\n",
      "47405.265625\n",
      "46982.4453125\n",
      "46953.87890625\n",
      "46887.58984375\n",
      "48559.7890625\n",
      "46763.01171875\n",
      "47354.95703125\n",
      "46984.515625\n",
      "46923.62890625\n",
      "47550.15234375\n",
      "47304.9765625\n",
      "47353.5078125\n",
      "46643.5390625\n",
      "46780.10546875\n",
      "47957.1484375\n",
      "47179.92578125\n",
      "46938.1015625\n",
      "46962.2734375\n",
      "46878.67578125\n",
      "48146.1875\n",
      "46677.7265625\n",
      "47776.31640625\n",
      "47223.8125\n",
      "47173.4296875\n",
      "47138.8359375\n",
      "46866.1015625\n",
      "47197.01953125\n",
      "46652.37890625\n",
      "46999.30859375\n",
      "47069.62890625\n",
      "46774.88671875\n",
      "47164.609375\n",
      "47348.41796875\n",
      "46600.72265625\n",
      "47493.984375\n",
      "47385.04296875\n",
      "47085.43359375\n",
      "47138.31640625\n",
      "47089.34765625\n",
      "47208.75390625\n",
      "47025.54296875\n",
      "47303.796875\n",
      "47864.69140625\n",
      "46910.3125\n",
      "47420.3046875\n",
      "46917.6796875\n",
      "47682.73046875\n",
      "47305.0\n",
      "48163.91796875\n",
      "46950.484375\n",
      "46642.8828125\n",
      "46879.30859375\n",
      "47235.04296875\n",
      "47017.01171875\n",
      "47220.078125\n",
      "46741.33984375\n",
      "47229.8046875\n",
      "46981.875\n",
      "47138.91796875\n",
      "47171.09375\n",
      "48265.50390625\n",
      "47849.578125\n",
      "46738.3359375\n",
      "46887.5546875\n",
      "46806.6953125\n",
      "47783.85546875\n",
      "46930.47265625\n",
      "46970.82421875\n",
      "46648.48046875\n",
      "47857.0\n",
      "46895.33984375\n",
      "47280.03515625\n",
      "47304.9765625\n",
      "47575.2421875\n",
      "46669.40234375\n",
      "47347.8828125\n",
      "47284.8359375\n",
      "47180.20703125\n",
      "46873.6328125\n",
      "46825.7578125\n",
      "46757.3828125\n",
      "47142.1171875\n",
      "47034.5859375\n",
      "46904.01171875\n",
      "46970.1328125\n",
      "46631.91015625\n",
      "47266.98046875\n",
      "46578.78515625\n",
      "47143.42578125\n",
      "46740.5\n",
      "47341.59375\n",
      "46477.55859375\n",
      "47148.546875\n",
      "46847.74609375\n",
      "46543.71484375\n",
      "47298.328125\n",
      "46676.92578125\n",
      "46922.92578125\n",
      "46981.0234375\n",
      "47399.8984375\n",
      "46994.58203125\n",
      "46741.6328125\n",
      "46860.58984375\n",
      "47718.46875\n",
      "47024.2734375\n",
      "47094.7265625\n",
      "46961.703125\n",
      "47235.80078125\n",
      "46733.828125\n",
      "46818.765625\n",
      "47567.69921875\n",
      "47846.58203125\n",
      "47064.734375\n",
      "46849.98828125\n",
      "46987.67578125\n",
      "46625.328125\n",
      "47279.53125\n",
      "46980.00390625\n",
      "46961.78125\n",
      "47173.3984375\n",
      "46943.3046875\n",
      "46681.328125\n",
      "46631.71484375\n",
      "46512.4296875\n",
      "47353.2890625\n",
      "47135.58203125\n",
      "46759.2421875\n",
      "47447.88671875\n",
      "47010.08984375\n",
      "47161.71875\n",
      "46735.0234375\n",
      "46553.88671875\n",
      "46890.89453125\n",
      "46727.390625\n",
      "46667.59375\n",
      "47157.7890625\n",
      "46756.48828125\n",
      "47097.9296875\n",
      "46674.69140625\n",
      "47237.52734375\n",
      "47609.421875\n",
      "46669.95703125\n",
      "46757.95703125\n",
      "46867.078125\n",
      "46606.10546875\n",
      "47520.828125\n",
      "47000.15234375\n",
      "46622.5546875\n",
      "46674.7890625\n",
      "46603.0703125\n",
      "46761.765625\n",
      "48336.67578125\n",
      "47311.9375\n",
      "46952.1640625\n",
      "47276.11328125\n",
      "47197.08984375\n",
      "47099.15625\n",
      "47817.12109375\n",
      "46911.22265625\n",
      "46618.71484375\n",
      "46812.8046875\n",
      "46703.2578125\n",
      "46769.5078125\n",
      "46731.0859375\n",
      "46680.92578125\n",
      "46724.2578125\n",
      "46739.68359375\n",
      "47086.9296875\n",
      "47199.25390625\n",
      "47082.05859375\n",
      "46733.04296875\n",
      "46818.53125\n",
      "46681.27734375\n",
      "46519.48046875\n",
      "47433.171875\n",
      "47331.26171875\n",
      "46637.46875\n",
      "46533.21875\n",
      "46830.87890625\n",
      "46963.32421875\n",
      "46593.78515625\n",
      "46803.26171875\n",
      "46861.4609375\n",
      "47080.3984375\n",
      "48002.58984375\n",
      "47015.78125\n",
      "46813.60546875\n",
      "47559.5390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46503.15625\n",
      "47153.5078125\n",
      "46804.84765625\n",
      "46754.0390625\n",
      "47140.56640625\n",
      "46596.66015625\n",
      "46823.828125\n",
      "46534.59765625\n",
      "46943.87890625\n",
      "46567.30859375\n",
      "46721.38671875\n",
      "46649.58984375\n",
      "46348.015625\n",
      "47112.51953125\n",
      "46565.390625\n",
      "46755.015625\n",
      "46947.3359375\n",
      "46714.76953125\n",
      "46757.05078125\n",
      "46612.1953125\n",
      "46910.3125\n",
      "46536.21484375\n",
      "46556.453125\n",
      "48261.1328125\n",
      "47061.54296875\n",
      "46825.75\n",
      "46967.6640625\n",
      "46645.421875\n",
      "47734.87109375\n",
      "46624.48828125\n",
      "46845.62109375\n",
      "46755.59765625\n",
      "46972.12109375\n",
      "46947.21875\n",
      "46772.1328125\n",
      "46894.359375\n",
      "46720.7265625\n",
      "46399.546875\n",
      "46726.109375\n",
      "47082.0546875\n",
      "46975.5\n",
      "46539.46484375\n",
      "46842.09765625\n",
      "46679.88671875\n",
      "47690.86328125\n",
      "46625.44140625\n",
      "46718.765625\n",
      "47061.54296875\n",
      "46827.06640625\n",
      "47065.73828125\n",
      "46717.09375\n",
      "47284.58203125\n",
      "46992.0859375\n",
      "46683.16796875\n",
      "46670.96875\n",
      "46934.76171875\n",
      "46961.40234375\n",
      "47168.96484375\n",
      "47375.26953125\n",
      "47146.4453125\n",
      "46936.53125\n",
      "47357.9375\n",
      "46653.96875\n",
      "46533.98046875\n",
      "46694.78515625\n",
      "46891.75\n",
      "46508.73046875\n",
      "47219.3125\n",
      "46808.4140625\n",
      "46590.515625\n",
      "47560.578125\n",
      "47573.70703125\n",
      "47131.2265625\n",
      "46706.98046875\n",
      "46596.09765625\n",
      "46870.72265625\n",
      "46462.5703125\n",
      "47046.9921875\n",
      "46706.921875\n",
      "46773.48046875\n",
      "46690.421875\n",
      "46639.65234375\n",
      "46874.7890625\n",
      "46688.21484375\n",
      "46855.71484375\n",
      "46577.12109375\n",
      "46987.78515625\n",
      "46745.68359375\n",
      "46835.6015625\n",
      "46775.94921875\n",
      "46375.125\n",
      "46841.88671875\n",
      "46585.81640625\n",
      "46801.95703125\n",
      "46485.56640625\n",
      "46680.5703125\n",
      "46627.02734375\n",
      "47253.375\n",
      "46951.50390625\n",
      "46959.99609375\n",
      "47091.546875\n",
      "46866.08203125\n",
      "46432.375\n",
      "46554.86328125\n",
      "46480.4765625\n",
      "46577.31640625\n",
      "46690.3671875\n",
      "46797.73828125\n",
      "46993.9140625\n",
      "46249.296875\n",
      "46752.14453125\n",
      "48125.03125\n",
      "46459.81640625\n",
      "47397.46875\n",
      "46882.26171875\n",
      "46498.2890625\n",
      "46505.046875\n",
      "46571.04296875\n",
      "46873.01171875\n",
      "46551.48046875\n",
      "46725.76171875\n",
      "46847.375\n",
      "47148.08203125\n",
      "46566.82421875\n",
      "46870.34375\n",
      "46813.328125\n",
      "46912.80859375\n",
      "46645.94921875\n",
      "46709.05859375\n",
      "46688.33984375\n",
      "46464.45703125\n",
      "46612.78515625\n",
      "46489.20703125\n",
      "46631.1953125\n",
      "46950.45703125\n",
      "46621.234375\n",
      "46527.40234375\n",
      "46692.1953125\n",
      "46675.3125\n",
      "46655.09765625\n",
      "47320.18359375\n",
      "47307.609375\n",
      "47256.87890625\n",
      "46669.8828125\n",
      "46623.28125\n",
      "46444.734375\n",
      "47338.71875\n",
      "46508.1328125\n",
      "46570.71484375\n",
      "46597.77734375\n",
      "46652.85546875\n",
      "46551.55859375\n",
      "46413.48828125\n",
      "46676.921875\n",
      "47119.77734375\n",
      "47038.9375\n",
      "47252.64453125\n",
      "46813.00390625\n",
      "47111.40625\n",
      "46452.06640625\n",
      "46541.25390625\n",
      "46715.34765625\n",
      "46668.96484375\n",
      "46819.7421875\n",
      "46665.60546875\n",
      "47738.2109375\n",
      "46699.61328125\n",
      "46816.5625\n",
      "46597.69921875\n",
      "46925.44140625\n",
      "46711.6015625\n",
      "46440.77734375\n",
      "46528.15625\n",
      "46582.5625\n",
      "46458.15234375\n",
      "46507.05078125\n",
      "47073.46484375\n",
      "46480.8828125\n",
      "46504.03515625\n",
      "46819.85546875\n",
      "47015.359375\n",
      "46875.41015625\n",
      "46459.12109375\n",
      "46972.41796875\n",
      "47047.45703125\n",
      "46494.66015625\n",
      "46670.96875\n",
      "46857.609375\n",
      "46932.94140625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-639a76a96848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 optimizer, loss=infer.TraceEnum_ELBO(max_iarange_nesting=1))\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/infer/traceenum_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrainable_params\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melbo_particle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0mloss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0melbo_particle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mloss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "import pyro.distributions as p\n",
    "from pyro import optim\n",
    "from pyro import infer\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "D = 20\n",
    "N = [len(d) for d in small_docs_id]\n",
    "K = 5\n",
    "V = max([max(d) for d in small_docs_id])\n",
    "print(f'D: {D} | V: {V} | K: {K}')\n",
    "\n",
    "#@pyro.poutine.broadcast\n",
    "def model(data):\n",
    "    # topic - word distribution\n",
    "    phi = pyro.sample(\"phi\", p.Dirichlet(torch.ones([K, V])).independent(1))\n",
    "  \n",
    "    for d in pyro.plate(\"documents\", D):\n",
    "        # document-topic distribution\n",
    "        theta_d = pyro.sample(f\"theta_{d}\", p.Dirichlet(torch.ones([K])))\n",
    "    \n",
    "        with pyro.plate(f\"words_{d}\", N[d]):\n",
    "            z = pyro.sample(f\"z_{d}\", p.Categorical(theta_d))\n",
    "            pyro.sample(f\"w_{d}\", p.Categorical(phi[z]), obs=data[d])\n",
    "            \n",
    "#@pyro.poutine.broadcast\n",
    "def guide(data):\n",
    "    beta_q = pyro.param(\"beta_q\", torch.ones([K, V]),constraint=constraints.positive)\n",
    "    phi_q = pyro.sample(\"phi\",p.Dirichlet(beta_q).independent(1))\n",
    "  \n",
    "    for d in pyro.plate(\"documents\", D):\n",
    "        alpha_q = pyro.param(f\"alpha_q_{d}\", torch.ones([K]),constraint=constraints.positive)\n",
    "        q_theta_d = pyro.sample(f\"theta_{d}\", p.Dirichlet(alpha_q))\n",
    "    \n",
    "        with pyro.plate(f\"words_{d}\", N[d]):\n",
    "            q_i = pyro.param(f\"q_{d}\", torch.randn([N[d], K]).exp(), constraint=constraints.simplex)\n",
    "            pyro.sample(f\"z_{d}\", p.Categorical(q_i))\n",
    "\n",
    "data = [torch.tensor(d).float() for d in small_docs_id]            \n",
    "            \n",
    "adam_params = {\"lr\": 0.01, \"betas\": (0.90, 0.999)}\n",
    "optimizer = optim.Adam(adam_params)\n",
    "\n",
    "# infer.config_enumerate(guide, 'parallel')\n",
    "pyro.clear_param_store()\n",
    "svi = infer.SVI(model, guide, optimizer, loss=infer.TraceEnum_ELBO(max_iarange_nesting=1))\n",
    "for _ in range(3000):\n",
    "    loss = svi.step(data)\n",
    "    print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- topic 0 -----\n",
      "  -- top words ---\n",
      "['tracer', '\\n\\t\\t\\t       ', 'incurring', 'rayshade', 'package', 'support', '\\n        ', 'chemist', 'image', 'graphics']\n",
      "---- topic 1 -----\n",
      "  -- top words ---\n",
      "['graphics', 'format', 'material', '\\n        ', 'crash', 'object', 'amiga', 'stuff', 'volunteer', 'soveriegn']\n",
      "---- topic 2 -----\n",
      "  -- top words ---\n",
      "['image', 'author', 'server', 'system', 'object', 'permit', 'explore', 'amiga', '\\n        ', \"qur'an\"]\n",
      "---- topic 3 -----\n",
      "  -- top words ---\n",
      "['package', 'available', 'image', 'buzzer', 'administration', 'file', 'server', 'graphics', 'blitter', 'include']\n",
      "---- topic 4 -----\n",
      "  -- top words ---\n",
      "['graphics', 'object', 'catagories', 'dynamic', 'ontario', 'shall', 'absolutely', 'scoring', 'renderers', 'image']\n"
     ]
    }
   ],
   "source": [
    "params = pyro.get_param_store()\n",
    "\n",
    "beta_q = params[\"beta_q\"]\n",
    "topic_words_distribution = p.Dirichlet(beta_q).sample()\n",
    "\n",
    "\n",
    "for t in range(K):\n",
    "    print(\"---- topic {} -----\".format(t))\n",
    "    print(\"  -- top words ---\")\n",
    "    top5_words = (torch.argsort(topic_words_distribution[t])[-10:]).cpu().numpy()\n",
    "    top5_words = list(map(lambda x: dictionary[x], reversed(\n",
    "        top5_words)))\n",
    "    print(top5_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tweets: 449334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/simi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/simi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% 50.0% "
     ]
    }
   ],
   "source": [
    "###### Twitter Data\n",
    "#####################\n",
    "#     LOAD DATA     #\n",
    "#####################\n",
    "\n",
    "import json_lines\n",
    "import csv\n",
    "\n",
    "def process_tweet(tweet):  \n",
    "    d = {}\n",
    "    d['hashtags'] = [hashtag['text'] for hashtag in tweet['entities']['hashtags']]\n",
    "    d['text'] = tweet['full_text']\n",
    "    d['user'] = tweet['user']['screen_name']\n",
    "    d['user_loc'] = tweet['user']['location']\n",
    "    d['created_at'] = tweet['created_at']\n",
    "    return d\n",
    "\n",
    "if False:\n",
    "    with open('congress_dataset/senators-1.jsonl', 'rb') as f:\n",
    "        with open(r'senators-1-tweets.csv', 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for item in json_lines.reader(f):\n",
    "                # Only collect tweets in English\n",
    "                if item['lang'] == 'en' and len(item['entities']['hashtags']) > 0:\n",
    "                    tweet_data = process_tweet(item)\n",
    "                    writer.writerow(list(tweet_data.values()))\n",
    "\n",
    "                    \n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"senators-1-tweets.csv\", header=None, names=['hashtags', 'text', 'user', 'user_location', 'created_at'])  \n",
    "print('num tweets: {}'.format(len(tweets)))\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = nlp(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "import random\n",
    "\n",
    "docs = []\n",
    "hashtags = []\n",
    "N = 2000\n",
    "rand_tweets = list(range(N)) #random.sample(range(len(tweets)), k=N)\n",
    "for i, tw in enumerate(rand_tweets):\n",
    "    if i % 1000 == 0:\n",
    "        print('{}%'.format(100./N*i), end=' ')\n",
    "    text = tweets.iloc[i]['text']\n",
    "    tokens = prepare_text_for_lda(text)\n",
    "    if random.random() > .9999:\n",
    "        print(tokens)\n",
    "    taggs = tweets.iloc[i]['hashtags'].replace('[', '').replace(']', '').replace('\\'', '').split(\",\")\n",
    "    hashtags.append([t.strip() for t in taggs])\n",
    "    docs.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = [' '.join(d) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(all_docs)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simi/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "no_topics = 10\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=15, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "today screen_name health getcovered visit county deadline tomorrow great healthcare\n",
      "Topic 1:\n",
      "family dreamer screen_name dreamactnow child country congress community goptaxscam americandreamer\n",
      "Topic 2:\n",
      "senate goptaxscam screen_name taxreform american business forward republican watch opportunity\n",
      "Topic 3:\n",
      "eclipse matter solar solareclipse montana mtpol angusontheroad pretty conservation lastbestoutdoorsfest\n",
      "Topic 4:\n",
      "celebrate happy world hanukkah wishing right courtsmatter light great friend\n",
      "Topic 5:\n",
      "rally accountable sesta jones birmingham final trafficker gotv4doug traffic training\n",
      "Topic 6:\n",
      "screen_name learn solareclipse2017 birthday alabama happy ussjohnsmccain rightsideofhistory visiting thought\n",
      "Topic 7:\n",
      "screen_name netneutrality internet savenetneutrality repeal protect economy today american fight\n",
      "Topic 8:\n",
      "dreamer screen_name thank standing family dreamactnow democrat earn solidarity 000\n",
      "Topic 9:\n",
      "screen_name today watch thanks support utpol great national discus honor\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
