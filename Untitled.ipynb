{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tweets: 449334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/simi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/simi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% 50.0% ['forward', 'hearing', 'tillerson', 'mattis', 'potus’', 'afghanistan', 'strategy', 'america', 'safe']\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "#     LOAD DATA     #\n",
    "#####################\n",
    "\n",
    "import json_lines\n",
    "import csv\n",
    "\n",
    "def process_tweet(tweet):  \n",
    "    d = {}\n",
    "    d['hashtags'] = [hashtag['text'] for hashtag in tweet['entities']['hashtags']]\n",
    "    d['text'] = tweet['full_text']\n",
    "    d['user'] = tweet['user']['screen_name']\n",
    "    d['user_loc'] = tweet['user']['location']\n",
    "    d['created_at'] = tweet['created_at']\n",
    "    return d\n",
    "\n",
    "if False:\n",
    "    with open('congress_dataset/senators-1.jsonl', 'rb') as f:\n",
    "        with open(r'senators-1-tweets.csv', 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for item in json_lines.reader(f):\n",
    "                # Only collect tweets in English\n",
    "                if item['lang'] == 'en' and len(item['entities']['hashtags']) > 0:\n",
    "                    tweet_data = process_tweet(item)\n",
    "                    writer.writerow(list(tweet_data.values()))\n",
    "\n",
    "                    \n",
    "import pandas as pd\n",
    "tweets = pd.read_csv(\"senators-1-tweets.csv\", header=None, names=['hashtags', 'text', 'user', 'user_location', 'created_at'])  \n",
    "print('num tweets: {}'.format(len(tweets)))\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = nlp(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "    \n",
    "\n",
    "import random\n",
    "\n",
    "docs = []\n",
    "hashtags = []\n",
    "N = 2000\n",
    "rand_tweets = list(range(N)) #random.sample(range(len(tweets)), k=N)\n",
    "for i, tw in enumerate(rand_tweets):\n",
    "    if i % 1000 == 0:\n",
    "        print('{}%'.format(100./N*i), end=' ')\n",
    "    text = tweets.iloc[i]['text']\n",
    "    tokens = prepare_text_for_lda(text)\n",
    "    if random.random() > .9999:\n",
    "        print(tokens)\n",
    "    taggs = tweets.iloc[i]['hashtags'].replace('[', '').replace(']', '').replace('\\'', '').split(\",\")\n",
    "    hashtags.append([t.strip() for t in taggs])\n",
    "    docs.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we should get only cleaned tweets\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "N_SMALL = N\n",
    "\n",
    "cleaned_tweetz = docs[:N_SMALL]\n",
    "\n",
    "bigram = gensim.models.Phrases(cleaned_tweetz)\n",
    "cleaned_tweetz = [bigram[t] for t in cleaned_tweetz]\n",
    "\n",
    "# create dictionary and corpus\n",
    "dictionary = Dictionary(cleaned_tweetz)\n",
    "\n",
    "# corpus = (token_id, count_in_curr_doc) , sparse representation\n",
    "corpus = [dictionary.doc2bow(clean_tween) for clean_tween in cleaned_tweetz]\n",
    "cleaned_tweetz_id = [dictionary.doc2idx(document=tw) for tw in cleaned_tweetz]\n",
    "\n",
    "# Cut all tweets after 5 words and remove tweets below 5 words\n",
    "cleaned_tweetz_id = [tw for tw in cleaned_tweetz_id if len(tw) >= 5]\n",
    "cleaned_tweetz_id = [tw[:5] for tw in cleaned_tweetz_id]\n",
    "cleaned_tweetz_id = torch.Tensor(np.array([np.array(tw, dtype=np.int32) for tw in cleaned_tweetz_id])).long()\n",
    "cleaned_tweetz_id = cleaned_tweetz_id.transpose(1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dictionary = Dictionary(hashtags)\n",
    "tags_id = [tag_dictionary.doc2idx(tag) for tag in hashtags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags: 725 | Number of Docs: 1899\n",
      "Creating MLP with sizes [4227, 100, 100, 10]\n",
      "Step\tLoss\n",
      "    0\t401365.0\n",
      "   10\t403034.0\n",
      "   20\t403725.75\n",
      "   30\t394284.28125\n",
      "   40\t402631.875\n",
      "   50\t390290.78125\n",
      "   60\t388546.96875\n",
      "   70\t388825.5\n",
      "   80\t393618.0625\n",
      "   90\t389916.5625\n",
      "  100\t382247.6875\n",
      "  110\t381889.625\n",
      "  120\t378190.625\n",
      "  130\t382897.0625\n",
      "  140\t375515.375\n",
      "  150\t379963.90625\n",
      "  160\t377358.78125\n",
      "  170\t375438.0625\n",
      "  180\t371513.0\n",
      "  190\t379264.6875\n",
      "  200\t379262.25\n",
      "  210\t374889.125\n",
      "  220\t379864.3125\n",
      "  230\t366250.90625\n",
      "  240\t368189.0625\n",
      "  250\t368085.375\n",
      "  260\t373838.78125\n",
      "  270\t369557.40625\n",
      "  280\t368699.78125\n",
      "  290\t363842.375\n",
      "  300\t360918.375\n",
      "  310\t365012.28125\n",
      "  320\t363589.0625\n",
      "  330\t358202.1875\n",
      "  340\t363491.21875\n",
      "  350\t361633.75\n",
      "  360\t372190.96875\n",
      "  370\t364865.96875\n",
      "  380\t361387.65625\n",
      "  390\t372541.09375\n",
      "  400\t357418.25\n",
      "  410\t358022.9375\n",
      "  420\t350235.625\n",
      "  430\t354729.5625\n",
      "  440\t359668.5625\n",
      "  450\t354494.5625\n",
      "  460\t355842.0\n",
      "  470\t355696.28125\n",
      "  480\t352970.3125\n",
      "  490\t353021.46875\n",
      "  500\t358836.3125\n",
      "  510\t352598.3125\n",
      "  520\t358078.53125\n",
      "  530\t352500.75\n",
      "  540\t348360.34375\n",
      "  550\t348074.53125\n",
      "  560\t346240.65625\n",
      "  570\t355103.8125\n",
      "  580\t349543.3125\n",
      "  590\t342965.9375\n",
      "  600\t345694.6875\n",
      "  610\t352380.34375\n",
      "  620\t346664.8125\n",
      "  630\t357998.375\n",
      "  640\t348954.1875\n",
      "  650\t349268.90625\n",
      "  660\t349792.375\n",
      "  670\t349821.65625\n",
      "  680\t348287.25\n",
      "  690\t342183.0\n",
      "  700\t343555.09375\n",
      "  710\t344865.5625\n",
      "  720\t341500.625\n",
      "  730\t341636.40625\n",
      "  740\t346283.0625\n",
      "  750\t350811.96875\n",
      "  760\t339686.65625\n",
      "  770\t346302.6875\n",
      "  780\t350025.5625\n",
      "  790\t344160.5625\n",
      "  800\t345972.0\n",
      "  810\t341077.28125\n",
      "  820\t342099.875\n",
      "  830\t341156.5625\n",
      "  840\t341789.09375\n",
      "  850\t342830.1875\n",
      "  860\t347896.875\n",
      "  870\t344259.40625\n",
      "  880\t346595.6875\n",
      "  890\t341211.75\n",
      "  900\t345648.0625\n",
      "  910\t348470.625\n",
      "  920\t335764.625\n",
      "  930\t350372.25\n",
      "  940\t340481.8125\n",
      "  950\t346138.96875\n",
      "  960\t345464.53125\n",
      "  970\t348088.8125\n",
      "  980\t338512.375\n",
      "  990\t349039.375\n",
      " 1000\t331269.0\n",
      " 1010\t338419.96875\n",
      " 1020\t344709.375\n",
      " 1030\t347261.625\n",
      " 1040\t346005.9375\n",
      " 1050\t330181.09375\n",
      " 1060\t343039.59375\n",
      " 1070\t345744.75\n",
      " 1080\t346621.375\n",
      " 1090\t343588.25\n",
      " 1100\t339258.6875\n",
      " 1110\t339242.0\n",
      " 1120\t341274.96875\n",
      " 1130\t341233.6875\n",
      " 1140\t342167.125\n",
      " 1150\t346635.0\n",
      " 1160\t340376.3125\n",
      " 1170\t340389.6875\n",
      " 1180\t342460.65625\n",
      " 1190\t337559.25\n",
      " 1200\t332455.5\n",
      " 1210\t337653.34375\n",
      " 1220\t345929.34375\n",
      " 1230\t332325.375\n",
      " 1240\t329779.5\n",
      " 1250\t335441.4375\n",
      " 1260\t336063.40625\n",
      " 1270\t337164.0625\n",
      " 1280\t344624.4375\n",
      " 1290\t334370.0\n",
      " 1300\t339902.28125\n",
      " 1310\t330670.75\n",
      " 1320\t331800.53125\n",
      " 1330\t341858.3125\n",
      " 1340\t330519.375\n",
      " 1350\t334742.4375\n",
      " 1360\t341419.96875\n",
      " 1370\t335198.5\n",
      " 1380\t338664.125\n",
      " 1390\t342187.65625\n",
      " 1400\t334605.875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a66f2b0d9387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step\\tLoss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_tweetz_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{: >5d}\\t{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/infer/traceenum_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \"\"\"\n\u001b[1;32m    364\u001b[0m         \u001b[0melbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m             \u001b[0melbo_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_dice_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_identically_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melbo_particle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/infer/traceenum_elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/infer/traceenum_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         model_trace, guide_trace = get_importance_trace(\n\u001b[0;32m--> 262\u001b[0;31m             \"flat\", self.max_plate_nesting, model, guide, *args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/infer/enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0magainst\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mguide_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     model_trace = poutine.trace(poutine.replay(model, trace=guide_trace),\n\u001b[1;32m     44\u001b[0m                                 graph_type=graph_type).get_trace(*args, **kwargs)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36mget_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mCalls\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mpoutine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mits\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m                                       args=args, kwargs=kwargs)\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/poutine/handlers.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m                                        escape_fn=functools.partial(escape_fn,\n\u001b[1;32m    465\u001b[0m                                                                    next_trace)))\n\u001b[0;32m--> 466\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mftr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mNonlocalExit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msite_container\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                     \u001b[0msite_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m                                       args=args, kwargs=kwargs)\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/poutine/messenger.py\u001b[0m in \u001b[0;36m_wraps\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0m_wraps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/poutine/messenger.py\u001b[0m in \u001b[0;36m_wraps\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0m_wraps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyro/poutine/messenger.py\u001b[0m in \u001b[0;36m_wraps\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0m_wraps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-a66f2b0d9387>\u001b[0m in \u001b[0;36mparametrized_guide\u001b[0;34m(predictor, data, tags, batch_size)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mdoc_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc_topics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "import functools\n",
    "from operator import itemgetter\n",
    "\n",
    "K = 10\n",
    "V = len(dictionary)\n",
    "V_per_doc = 5\n",
    "D = cleaned_tweetz_id.shape[1]\n",
    "M = len(tag_dictionary)\n",
    "l_sizes = '100-100'\n",
    "\n",
    "print(f'Number of tags: {M} | Number of Docs: {D}')\n",
    "\n",
    "# This is a fully generative model of a batch of documents.\n",
    "# data is a [num_words_per_doc, num_documents] shaped array of word ids\n",
    "# (specifically it is not a histogram). We assume in this simple example\n",
    "# that all documents have the same number of words.\n",
    "def model(data=None, tags=None, batch_size=None):\n",
    "    # Globals.\n",
    "    with pyro.plate(\"topics\", K):\n",
    "        topic_weights = pyro.sample(\"topic_weights\", dist.Gamma(1. / K, 1.))\n",
    "        topic_words = pyro.sample(\"topic_words\", dist.Dirichlet(torch.ones(V) / V))\n",
    "        topic_tags_distr = pyro.sample(\"topic_tags\", dist.Dirichlet(torch.ones(M) / M))\n",
    "\n",
    "    # Locals.\n",
    "    with pyro.plate(\"documents\", D) as ind:\n",
    "        if data is not None:\n",
    "            with pyro.util.ignore_jit_warnings():\n",
    "                assert data.shape == (V_per_doc, D)\n",
    "            data = data[:, ind]\n",
    "            tags = torch.tensor([tags[int(id)][0] for id in ind]).float()\n",
    "\n",
    "        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(topic_weights))\n",
    "        with pyro.plate(\"words\", V_per_doc):\n",
    "            # The word_topics variable is marginalized out during inference,\n",
    "            # achieved by specifying infer={\"enumerate\": \"parallel\"} and using\n",
    "            # TraceEnum_ELBO for inference. Thus we can ignore this variable in\n",
    "            # the guide.\n",
    "            word_topics = pyro.sample(\"word_topics\", dist.Categorical(doc_topics),\n",
    "                                      infer={\"enumerate\": \"parallel\"})\n",
    "            data = pyro.sample(\"doc_words\", dist.Categorical(topic_words[word_topics]),\n",
    "                               obs=data)\n",
    "\n",
    "        # sample 1 topic\n",
    "        tag_topic = pyro.sample(\"tag_topic\", dist.Categorical(doc_topics), infer={\"enumerate\": \"parallel\"})\n",
    "        if tags is not None:\n",
    "            tag = pyro.sample(\"tag\", dist.Categorical(topic_tags_distr[tag_topic]), obs=tags)\n",
    "\n",
    "    return topic_weights, topic_words, data, tag\n",
    "\n",
    "\n",
    "# We will use amortized inference of the local topic variables, achieved by a\n",
    "# multi-layer perceptron. We'll wrap the guide in an nn.Module.\n",
    "def make_predictor():\n",
    "    layer_sizes = ([V] +\n",
    "                   [int(s) for s in l_sizes.split('-')] +\n",
    "                   [K])\n",
    "    print('Creating MLP with sizes {}'.format(layer_sizes))\n",
    "    layers = []\n",
    "    for in_size, out_size in zip(layer_sizes, layer_sizes[1:]):\n",
    "        layer = nn.Linear(in_size, out_size)\n",
    "        layer.weight.data.normal_(0, 0.001)\n",
    "        layer.bias.data.normal_(0, 0.001)\n",
    "        layers.append(layer)\n",
    "        layers.append(nn.Sigmoid())\n",
    "    layers.append(nn.Softmax(dim=-1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def parametrized_guide(predictor, data, tags, batch_size=None):\n",
    "    # Use a conjugate guide for global variables.\n",
    "    topic_weights_posterior = pyro.param(\n",
    "        \"topic_weights_posterior\",\n",
    "        lambda: torch.ones(K),\n",
    "        constraint=constraints.positive)\n",
    "    topic_words_posterior = pyro.param(\n",
    "        \"topic_words_posterior\",\n",
    "        lambda: torch.ones(K, V),\n",
    "        constraint=constraints.greater_than(0.5))\n",
    "    topic_tags_posterior = pyro.param(\n",
    "        \"topic_tags_posterior\",\n",
    "        lambda: torch.ones(K, M),\n",
    "        constraint=constraints.greater_than(0.5))\n",
    "    with pyro.plate(\"topics\", K):\n",
    "        pyro.sample(\"topic_weights\", dist.Gamma(topic_weights_posterior, 1.))\n",
    "        pyro.sample(\"topic_words\", dist.Dirichlet(topic_words_posterior))\n",
    "        pyro.sample(\"topic_tags\", dist.Dirichlet(topic_tags_posterior))\n",
    "\n",
    "    # Use an amortized guide for local variables.\n",
    "    pyro.module(\"predictor\", predictor)\n",
    "    with pyro.plate(\"documents\", D, batch_size) as ind:\n",
    "        # The neural network will operate on histograms rather than word\n",
    "        # index vectors, so we'll convert the raw data to a histogram.\n",
    "        if torch._C._get_tracing_state():\n",
    "            counts = torch.eye(1024)[data[:, ind]].sum(0).t()\n",
    "        else:\n",
    "            counts = torch.zeros(V, ind.size(0))\n",
    "            counts.scatter_add_(0, data[:, ind], torch.tensor(1.).expand(counts.shape))\n",
    "        doc_topics = predictor(counts.transpose(0, 1))\n",
    "        pyro.sample(\"doc_topics\", dist.Delta(doc_topics, event_dim=1))\n",
    "\n",
    "\n",
    "pyro.set_rng_seed(0)\n",
    "pyro.clear_param_store()\n",
    "# pyro.enable_validation(True)\n",
    "\n",
    "# We can generate synthetic data directly by calling the model.\n",
    "#true_topic_weights, true_topic_words, data = model()\n",
    "\n",
    "# We'll train using SVI.\n",
    "predictor = make_predictor()\n",
    "guide = functools.partial(parametrized_guide, predictor)\n",
    "Elbo = TraceEnum_ELBO  # JitTraceEnum_ELBO if args.jit else TraceEnum_ELBO\n",
    "elbo = Elbo(max_plate_nesting=2)\n",
    "optim = Adam({'lr': 1e-2})\n",
    "svi = SVI(model, guide, optim, elbo)\n",
    "print('Step\\tLoss')\n",
    "for step in range(5000):\n",
    "    loss = svi.step(cleaned_tweetz_id, tags_id, batch_size=64)\n",
    "    if step % 10 == 0:\n",
    "        print('{: >5d}\\t{}'.format(step, loss))\n",
    "loss = elbo.loss(model, guide, data)\n",
    "print('final loss = {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- topic 0 -----\n",
      "['today', 'thank', 'netneutrality', 'SCREEN_NAME', 'repeal_netneutrality', 'congress', 'goptaxscam', 'family', 'would', 'watch']\n",
      "---- topic 1 -----\n",
      "['SCREEN_NAME', 'internet', 'american', 'thanks', 'today', 'republican', 'great', 'dreamer', 'family', 'netneutrality']\n",
      "---- topic 2 -----\n",
      "['SCREEN_NAME', 'dreamer_democrat', 'family', 'friend', 'community', 'visit', 'senator', 'join', 'young', 'years']\n",
      "---- topic 3 -----\n",
      "['people', 'SCREEN_NAME', 'taxreform', 'dreamer', 'republican', 'speaking', 'netneutrality', 'great', 'congress', 'today']\n",
      "---- topic 4 -----\n",
      "['thank', 'support', 'internet', 'today', 'SCREEN_NAME', 'dreamer', 'taxreform', 'republican', 'congress', 'community']\n",
      "---- topic 5 -----\n",
      "['SCREEN_NAME', 'visit', 'sign', 'bipartisan', 'dreamactnow_family', 'hatch', 'honor', 'senator', 'today', 'national']\n",
      "---- topic 6 -----\n",
      "['today', 'goptaxscam', 'congress', 'dreamer', 'netneutrality', 'fight', 'SCREEN_NAME', 'senate', 'family', 'internet']\n",
      "---- topic 7 -----\n",
      "['republican', 'would', 'SCREEN_NAME', 'speaking', 'today', 'thanks', 'proud', 'repeal_netneutrality', 'dreamer', 'american']\n",
      "---- topic 8 -----\n",
      "['SCREEN_NAME', 'great', 'netneutrality', 'senate', 'dreamer', 'goptaxscam', 'thanks', 'today', 'american', 'republican']\n",
      "---- topic 9 -----\n",
      "['another', 'continue', 'business', 'standing_solidarity', 'icymi', 'celebrate', 'SCREEN_NAME', 'state', 'protect', 'americandreamer']\n"
     ]
    }
   ],
   "source": [
    "params = pyro.get_param_store()\n",
    "\n",
    "words_per_topic_distr = params['topic_words_posterior']\n",
    "\n",
    "for t in range(K):\n",
    "    print(\"---- topic {} -----\".format(t))\n",
    "    top5_words = (torch.argsort(words_per_topic_distr[t])[-10:]).cpu().numpy()\n",
    "    top5_words = list(map(lambda x: dictionary[x], reversed(top5_words)))\n",
    "    print(top5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, tags = model(cleaned_tweetz_id, tags_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calling',\n",
       " 'SCREEN_NAME',\n",
       " 'chairman',\n",
       " 'SCREEN_NAME',\n",
       " 'abandon',\n",
       " 'reckless',\n",
       " 'netneutrality',\n",
       " 'future',\n",
       " 'internet',\n",
       " 'hang',\n",
       " 'balance',\n",
       " 'savenetneutrality']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweetz[299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 tweet: Kyle Duncan defended a law that would have made abortion nearly impossible to access in Texas. The Supreme Court disagreed and overturned the law. He’s #BadForWomen #CourtsMatter \n",
      "| tag: BadForWomen\n",
      "\n",
      "real tags: ['BadForWomen', 'CourtsMatter']\n",
      "\n",
      "\n",
      "401 tweet: Don Willett said he “resisted” the idea that glass ceilings, pay equity, sexual discrimination and harassment were serious problems for women.  #BadforWomen #CourtsMatter \n",
      "| tag: BadforWomen\n",
      "\n",
      "real tags: ['BadforWomen', 'CourtsMatter']\n",
      "\n",
      "\n",
      "402 tweet: Steven Grasz defended state efforts to deny Medicaid coverage of abortion to women who are raped, which is required by federal law. #BadForWomen #CourtsMatter \n",
      "| tag: BadForWomen\n",
      "\n",
      "real tags: ['BadForWomen', 'CourtsMatter']\n",
      "\n",
      "\n",
      "403 tweet: Our courts have a critical role in upholding women’s constitutional and civil rights, from health care to workplace discrimination. We must speak out against judges who are #BadforWomen. #CourtsMatter \n",
      "| tag: BadforWomen\n",
      "\n",
      "real tags: ['BadforWomen', 'CourtsMatter']\n",
      "\n",
      "\n",
      "404 tweet: RT @CAL_FIRE: #ThomasFire [update] Hwy 150 and Hwy 126, north of Santa Paula (Ventura County) is now 234,200 acres and 20% contained. Unifi… \n",
      "| tag: ThomasFire\n",
      "\n",
      "real tags: ['ThomasFire']\n",
      "\n",
      "\n",
      "405 tweet: RT @CaltransDist7: #ThomasFire @CountyVentura highway closure update. There are additional closures along State Route 192 from Carpinteria… \n",
      "| tag: ThomasFire\n",
      "\n",
      "real tags: ['ThomasFire']\n",
      "\n",
      "\n",
      "406 tweet: RT @ReadyLA: Local Assistance Center for #CreekFire #RyeFire and #SkirballFire opens today at Lake View Terrace Rec Center! We'll be here 1… \n",
      "| tag: CreekFire\n",
      "\n",
      "real tags: ['CreekFire', 'RyeFire', 'SkirballFire']\n",
      "\n",
      "\n",
      "407 tweet: RT @countyofsb: This map shows the progression of the #ThomasFire through last night. The fire grew yesterday by 7,671 acres. https://t.co/… \n",
      "| tag: ThomasFire\n",
      "\n",
      "real tags: ['ThomasFire']\n",
      "\n",
      "\n",
      "408 tweet: Heading to the Senate floor to speak in opposition to the Trump Administration’s plan to end #netneutrality. We need to keep the Internet open, free, and with equal access for all.  Watch live: https://t.co/KbOVoY2cXD \n",
      "| tag: netneutrality\n",
      "\n",
      "real tags: ['netneutrality']\n",
      "\n",
      "\n",
      "409 tweet: Thanks @JimmyKimmel, you’re absolutely right. Children’s health care is NOT negotiable. Congress needs to reauthorize CHIP funding immediately. It’s a no brainer! #FundCHIPNow https://t.co/bI5Ih1UIh8 \n",
      "| tag: FundCHIPNow\n",
      "\n",
      "real tags: ['FundCHIPNow']\n",
      "\n",
      "\n",
      "410 tweet: Happy #Hanukkah to all those beginning their celebrations tonight! Wishing you a warm holiday filled with light, peace, joy, hope, and happiness. \n",
      "| tag: Hanukkah\n",
      "\n",
      "real tags: ['Hanukkah']\n",
      "\n",
      "\n",
      "411 tweet: The #GOPTaxPlan undercuts America’s electric vehicle and renewable energy transformation. It would be senseless for Republicans to disrupt these new industries that are a key part of growing our economy and fighting climate change. \n",
      "| tag: GOPTaxPlan\n",
      "\n",
      "real tags: ['GOPTaxPlan']\n",
      "\n",
      "\n",
      "412 tweet: I asked Brett Talley last month about his failure to disclose potential conflicts of interest and online commentary. He never responded to my letter. https://t.co/1fH6yXNwiS #CourtsMatter \n",
      "| tag: CourtsMatter\n",
      "\n",
      "real tags: ['CourtsMatter']\n",
      "\n",
      "\n",
      "413 tweet: RT @GOPHELP: At 10 AM EST, the Senate HELP committee will hold the 3rd hearing on implementing 21st Century #Cures. Today, the focus will b… \n",
      "| tag: Cures\n",
      "\n",
      "real tags: ['Cures']\n",
      "\n",
      "\n",
      "414 tweet: Every Senate Republican voted last night to confirm Steven Grasz to a lifetime judgeship despite the fact that he was voted unanimously “NOT QUALIFIED” due to concerns that he could not be impartial. Biased judges on our courts should alarm every American. #CourtsMatter \n",
      "| tag: CourtsMatter\n",
      "\n",
      "real tags: ['CourtsMatter']\n",
      "\n",
      "\n",
      "415 tweet: Alexander at HELP’s 3rd oversight hearing on 21st Century #Cures: Early intervention in mental health is critical. https://t.co/ncoM7gJOv8 \n",
      "| tag: Cures\n",
      "\n",
      "real tags: ['Cures']\n",
      "\n",
      "\n",
      "416 tweet: You can’t make this stuff up: After calls from Pres Trump’s wealthy friends, Republicans will alter the corporate rate in #GOPTaxScam... to provide an extra tax cut for the wealthiest individuals. #ScroogeWouldBeProud #GOPShouldBeAshamed\n",
      "https://t.co/w4QGuPl8dN \n",
      "| tag: GOPTaxScam\n",
      "\n",
      "real tags: ['GOPTaxScam', 'ScroogeWouldBeProud', 'GOPShouldBeAshamed']\n",
      "\n",
      "\n",
      "417 tweet: Joining @Hallerin on @987news at 4:15 PM ET to talk about how #TaxReform will help Tennesseans keep more of their paycheck in their pockets. Listen here: https://t.co/acTWFLuhua \n",
      "| tag: TaxReform\n",
      "\n",
      "real tags: ['TaxReform']\n",
      "\n",
      "\n",
      "418 tweet: RT @IanDon: .@SenJackReed calls Jones' win in #AlabamaSenateElection \"a victory of common sense and common decency\" https://t.co/d7EBG6Y2DK \n",
      "| tag: AlabamaSenateElection\n",
      "\n",
      "real tags: ['AlabamaSenateElection']\n",
      "\n",
      "\n",
      "419 tweet: It’s not too late.  Tell the Trump Admin not to repeal #NetNeutrality. The Internet should be open to all…\n",
      "https://t.co/BBKbMjBgPu \n",
      "| tag: NetNeutrality\n",
      "\n",
      "real tags: ['NetNeutrality']\n",
      "\n",
      "\n",
      "420 tweet: 5 years later &amp; our hearts still ache for #Newtown. These kids should be sitting in their 6th grade classrooms today w/ their teachers, but their lives were cut short by #gunviolence. Congress needs to act &amp; it can start by ending federal ban on gun violence research. #SandyHook https://t.co/nzlPnAdDEv \n",
      "| tag: Newtown\n",
      "\n",
      "real tags: ['Newtown', 'gunviolence', 'SandyHook']\n",
      "\n",
      "\n",
      "421 tweet: The Trump Admin just voted to repeal #NetNeutrality. But this isn't over. \n",
      " \n",
      "We will work in Congress to invalidate the #FCC’s action &amp; keep the Internet open &amp; free for all.\n",
      " \n",
      "The vast majority of Americans support Net Neutrality. 3 FCC votes will not change that. \n",
      "| tag: NetNeutrality\n",
      "\n",
      "real tags: ['NetNeutrality', 'FCC']\n",
      "\n",
      "\n",
      "422 tweet: FCC vote to repeal #NetNeutrality could be start of more restrictive access to the Internet. We can’t let that happen. Congress can overturn this decision &amp; ensure the Internet remains open &amp; free. Contact your Sens. Tell them to support a CRA invalidating FCC’s ruling. https://t.co/OsR44YjDJB \n",
      "| tag: NetNeutrality\n",
      "\n",
      "real tags: ['NetNeutrality']\n",
      "\n",
      "\n",
      "423 tweet: RT @senrobportman: Thank you @SenBlumenthal, @SenJohnMcCain, @clairecmc, @JohnCornyn, &amp; @SenatorHeitkamp for supporting #SESTA &amp; its missio… \n",
      "| tag: SESTA\n",
      "\n",
      "real tags: ['SESTA']\n",
      "\n",
      "\n",
      "424 tweet: School districts from coast to coast need help to fix and build schools. Instead of  #GOPTaxScam, we need a federal infrastructure bill that includes real funding to help improve schools. That’s why I’ve introduced the School Building Improvement Act.\n",
      "\n",
      "https://t.co/WPHuuRbudt \n",
      "| tag: GOPTaxScam\n",
      "\n",
      "real tags: ['GOPTaxScam']\n",
      "\n",
      "\n",
      "425 tweet: The #GOPTaxScam is an American job killer!\n",
      "\n",
      "It's larded w/ incentives for corporations that ship American jobs overseas. But it eliminates tax relief working families rely on to help pay for college, health care, retirement, etc.\n",
      "\n",
      "https://t.co/5WP0HT3Rsu \n",
      "| tag: GOPTaxScam\n",
      "\n",
      "real tags: ['GOPTaxScam']\n",
      "\n",
      "\n",
      "426 tweet: Getting set to join RI delegation, pediatricians, &amp; children's health advocates at @MeetingStreetRI to urge GOP to #ExtendCHIP to ensure health coverage for all children. #FundCHIPNow Watch live at: https://t.co/qppTMahUaO \n",
      "| tag: ExtendCHIP\n",
      "\n",
      "real tags: ['ExtendCHIP', 'FundCHIPNow']\n",
      "\n",
      "\n",
      "427 tweet: RT @SenatorMenendez: We must pass the #DreamAct before the holidays for ppl like Erika, an #AmericanDreamer whom the gov't promised would b… \n",
      "| tag: DreamAct\n",
      "\n",
      "real tags: ['DreamAct', 'AmericanDreamer']\n",
      "\n",
      "\n",
      "428 tweet: TODAY IN LAS VEGAS: \n",
      "\n",
      "@NVHealthLink is hosting a Holiday Health Fair for the last day of open enrollment today from Noon-8pm at St. Rose Dominican Hospital. Nevadans who need in-person assistance signing-up for healthcare, please stop by. They're there to help! #GetCoveredNow https://t.co/y4TqvML3VE \n",
      "| tag: GetCoveredNow\n",
      "\n",
      "real tags: ['GetCoveredNow']\n",
      "\n",
      "\n",
      "429 tweet: 🗣️ SPREAD THE WORD:\n",
      "You’re invited to Claire’s 50th Public #ShowMeTownHalls of 2017!\n",
      "\n",
      "WHERE: Kirkwood\n",
      "WHEN: Tomorrow—Saturday, Dec. 16 at 10 AM\n",
      "DETAILS ➡️ https://t.co/b7vBkC4YC9 https://t.co/BDktkgmeyG \n",
      "| tag: ShowMeTownHalls\n",
      "\n",
      "real tags: ['ShowMeTownHalls']\n",
      "\n",
      "\n",
      "430 tweet: TODAY IN RENO:\n",
      "\n",
      "@NVHealthLink is hosting a Holiday Health Fair for the last day of open enrollment today from Noon-8pm at the Reno-Sparks Convention Center. Nevadans who need in-person assistance signing-up for healthcare, please stop by. They're there to help! #GetCoveredNow https://t.co/p58aho2HbB \n",
      "| tag: GetCoveredNow\n",
      "\n",
      "real tags: ['GetCoveredNow']\n",
      "\n",
      "\n",
      "431 tweet: Astrid, from Las Vegas, is one of the 12,000 #Dreamers in NV that makes our communities stronger. We must protect Dreamers like her, keep families together, and give them the peace of mind they've earned. It's past time the GOP bring up and pass the #DreamActNow. #AmericanDreamer https://t.co/oFtH5o5NXN \n",
      "| tag: Dreamers\n",
      "\n",
      "real tags: ['Dreamers', 'DreamActNow', 'AmericanDreamer']\n",
      "\n",
      "\n",
      "432 tweet: RT @MartinHeinrich: If #AmericanDreamer &amp; ABQ educator Oriandi loses #DACA she won’t be able to continue serving students &amp; families. We ne… \n",
      "| tag: AmericanDreamer\n",
      "\n",
      "real tags: ['AmericanDreamer', 'DACA']\n",
      "\n",
      "\n",
      "433 tweet: RT @SenJeffMerkley: @realDonaldTrump Meet Grecia – an #AmericanDREAMer who went to the same high school I did in East Portland. She is a st… \n",
      "| tag: AmericanDREAMer\n",
      "\n",
      "real tags: ['AmericanDREAMer']\n",
      "\n",
      "\n",
      "434 tweet: RT @RonWyden: Today, Oregon #Dreamer Juan Navarro shared his story and why he believes we need a #DreamActNow. I am proud to #StandWithDrea… \n",
      "| tag: Dreamer\n",
      "\n",
      "real tags: ['Dreamer', 'DreamActNow']\n",
      "\n",
      "\n",
      "435 tweet: RT @RonWyden: These Oregonians visited today to share why we need a #DreamActNow. https://t.co/fbaPGIBmQl \n",
      "| tag: DreamActNow\n",
      "\n",
      "real tags: ['DreamActNow']\n",
      "\n",
      "\n",
      "436 tweet: RT @jlabomb: Accountability, with a Christmas flare. @clairecmc kicks off her 50th (not a typo) public #MO town hall of 2017, down the road… \n",
      "| tag: MO\n",
      "\n",
      "real tags: ['MO']\n",
      "\n",
      "\n",
      "437 tweet: A big thank you to ~everyone~ who took time out of their Saturday morning to talk #TaxBill, #NetNeutrality, #Healthcare, and more at Claire's 50th town hall of 2017! Great Q's, great crowd. #ShowMeTownHalls 🇺🇸 https://t.co/U0lJOa7Nvd \n",
      "| tag: TaxBill\n",
      "\n",
      "real tags: ['TaxBill', 'NetNeutrality', 'Healthcare', 'ShowMeTownHalls']\n",
      "\n",
      "\n",
      "438 tweet: #Dreamers live in our communities, go to our churches, shop in our grocery stores, attend our schools, are our neighbors and our friends. Why would we make them live a dual life? Why would we treat them differently? Why would we kick them out of their homes? #DreamActNow \n",
      "| tag: Dreamers\n",
      "\n",
      "real tags: ['Dreamers', 'DreamActNow']\n",
      "\n",
      "\n",
      "439 tweet: Aaron is one of over 13,000 Nevada #Dreamers who is at risk of deportation because of @realDonaldTrump's heartless decision to rescind #DACA. Congress must pass the #DreamActNow to keep families together. #AmericanDreamer https://t.co/i7WYYsDvxq \n",
      "| tag: Dreamers\n",
      "\n",
      "real tags: ['Dreamers', 'DACA', 'DreamActNow', 'AmericanDreamer']\n",
      "\n",
      "\n",
      "440 tweet: TODAY IN ELKO: \n",
      "\n",
      "Friday, 12/15, is the last day to sign-up for health insurance during open enrollment. @NVHealthLink can help you enroll or answer any questions today in Elko from 2-7pm at the Griswold Hall. \n",
      "\n",
      "#GetCovered, Nevada! https://t.co/lSlHjiAXKA \n",
      "| tag: GetCovered\n",
      "\n",
      "real tags: ['GetCovered']\n",
      "\n",
      "\n",
      "441 tweet: RT @MartinHeinrich: No member of Congress should rest easy until #Dreamers are able to live without fear in the only nation they call home.… \n",
      "| tag: Dreamers\n",
      "\n",
      "real tags: ['Dreamers']\n",
      "\n",
      "\n",
      "442 tweet: RT @SenWhitehouse: Rodrigo came to the US from Portugal at 10 months old. Raised in the US, he doesn't remember Portugal at all. #DACA allo… \n",
      "| tag: DACA\n",
      "\n",
      "real tags: ['DACA']\n",
      "\n",
      "\n",
      "443 tweet: RT @biz: Calling on Congress to pass the #DreamAct. Because we are all Dreamers. https://t.co/Y8D10E88OM https://t.co/LCc1Y70zFz \n",
      "| tag: DreamAct\n",
      "\n",
      "real tags: ['DreamAct']\n",
      "\n",
      "\n",
      "444 tweet: RT @SenJeffMerkley: I told the story of Diana, a young #AmericanDREAMer from Salem, OR. She is a cancer survivor &amp; is working to save lives… \n",
      "| tag: AmericanDREAMer\n",
      "\n",
      "real tags: ['AmericanDREAMer']\n",
      "\n",
      "\n",
      "445 tweet: Maria from Reno, NV, is one of over 13,000 brilliant, hardworking #Dreamers who calls Nevada home. Dreamers like Maria is what helps make our communities stronger. \n",
      "\n",
      "Congress must pass #DreamActNow and protect people like Maria from being deported from her home. #AmericanDREAMer https://t.co/eBnhMvG3W1 \n",
      "| tag: Dreamers\n",
      "\n",
      "real tags: ['Dreamers', 'DreamActNow', 'AmericanDREAMer']\n",
      "\n",
      "\n",
      "446 tweet: RT @HispanicCaucus: As of TODAY, 11,790 #DACA recipients have lost their status after Trump ended the program on Sept 5th. #Dreamers can’t… \n",
      "| tag: DACA\n",
      "\n",
      "real tags: ['DACA', 'Dreamers']\n",
      "\n",
      "\n",
      "447 tweet: Dreamers are our nation’s teachers, engineers, leaders, business owners, future leaders and so much more; they are our neighbors and our friends. We must protect them from deportation by passing the #DreamActNow #AmericanDREAMer \n",
      "| tag: DreamActNow\n",
      "\n",
      "real tags: ['DreamActNow', 'AmericanDREAMer']\n",
      "\n",
      "\n",
      "448 tweet: Trump’s decision to deport #DREAMers is not guided by sound policy, but by xenophobia &amp; myths. Deporting them robs our communities of their contributions and betrays America’s immigrant heritage. We must put partisanship aside and pass the #DreamActNow before the end of the year. \n",
      "| tag: DREAMers\n",
      "\n",
      "real tags: ['DREAMers', 'DreamActNow']\n",
      "\n",
      "\n",
      "449 tweet: From fighting to repeal \"Don't Ask, Don't Tell\", to ensuring healthcare for 9/11 first responders, &amp; helping to implement campus sexual assault reforms - @SenGillibrand is a leader, whose voice is needed. \n",
      "\n",
      "The only lightweight here is this #failing President, @realDonaldTrump. https://t.co/TlYC05ud2u \n",
      "| tag: failing\n",
      "\n",
      "real tags: ['failing']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(400, 450):\n",
    "    print(\"{} tweet: {} \\n| tag: {}\\n\".format(i, tweets.iloc[i].text, tag_dictionary[int(tags[i])]))\n",
    "    print('real tags: {}\\n\\n'.format(tweets.iloc[i].hashtags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
