\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}	% For figure environment
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{framed}
\usepackage{mathtools}
\usepackage{amssymb}


\newcommand\R{\mathbb{R}}
\newcommand\todo[1]{\textcolor{red}{#1}}

\begin{document}
\title{Write-Up Robot Judge}

\author{
  Huber Simon\\
  Department of Computer Science, ETH Zurich, Switzerland
}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
Topic models are designed to group documents with similar content. A lot of this techniques are unsupervised for multiple reasons: \begin{enumerate}
	\item There is just not enough data that is already labeled. 
	\item The target data set is very small or very specific. 
\end{enumerate} 
But if we interpret hashtags as labeled data for topics then there is already enough data labeled for a very wide range of topics on twitter. With 500 million tweets per day \cite{twitter_stats}. \todo{Figure out how many percent are tagged with a topic.}. The idea then was simple, learn a topic model \todo{explain topic models} through hashtags and apply it to other domains. In this work we were interested in applying it to speeches of politicians. In particular we thought about the following ideas: How does the topics of a speech for a senator change if he is up for reelection ? How are the topics distributed for a democrat or for a republican senator ? Can we use hash tags to predict the party ? And lastly how do the topics compare between speech and twitter for a specific politician and to the general trendy topics ? \\
The paper continuous in the following manor: In section \ref{sec:Plan} we discuss the two models we used to predict the hash tags and why one failed. Then in section \ref{sec:Experiments} we described the planned and executed experiments as well as their result. In section \ref{sec:Discussion} we discuss the results and finally in section \ref{sec:Summary} we summarize and conclude the paper. 

- General Idea
- Hypothesis 
- Paper Structure


\section{Data}
\label{sec:Data}
In this section we present the used datasets, how they were obtained and how they looked. 
\subsection{Twitter Data}
Twitter has the rule that one is not allowed to save tweets along with text. This is because if one wants to make any changes to the tweets e.g. delete them etc. this is no longer globally possible if they are saved somewhere else. So one is only allowed to save them via an unique ID that can be used to retrieve the tweets with the twitter api. A databank full of this tweet ID datasets is found in \cite{twitter_datasets}. We downloaded two datasets.
\subsubsection{Congress Dataset}
The 115th U.S. Congress Tweet Ids dataset contains 2,041,399 tweets. It covers a date distance from 2017-01-27 to 2019-01-02. It therefore covers 1 year before the senate elections and one year after the senate elections which makes it definitely very interesting for us. 
\subsubsection{News Dataset}
The News Outlet Tweet Ids we also downloaded was a fallback dataset which in case the congress dataset was to "fuzzy" could be used to train on more well written, structured and diverse topic distributed dataset. Which was of course just an assumption. 
\subsection{Speeches Data}
The congressional records and therefore the speeches can all be publicly accessed on the congress page \cite{congress_page}. But this side makes it very hard to access specific information. So people developed software packages that parse this side for the information and presents it in an accessible format mostly json. I first tried a package from another student but due to quite a few bugs I had to switch to \cite{congress-downloader} which worked very nicely. I used it to parse all congress information from the years 2017 and 2018. 


\section{Plan}
\label{sec:Plan}
In the first part of this section I will tell shortly about the failure and massive time loss that resulted in trying to adapt LDA to be used with the twitter data and in the second part a short summary of the neural network based method that finally worked for predicting hash tags. 
\subsection{Latent Dirichlet Allocation (LDA)}
\subsubsection{Standard LDA}
LDA was one of the algorithms for topic models that got presented in class. The generative model to generate a word makes it very simple to think about. Given a document sample a topic and for this topic sample a word. And the word is the thing we observe and we can use to learn the corresponding distributions. \\
\textbf{More formally} the process depicted in Figure \ref{fig:ldaplate} is defined as: 

\small
\begin{align*}
&\textit{Given } \alpha, \beta \\
&\textit{Sample topic-distribution: } \theta_m \sim Dir(\alpha)\\
&\textit{Sample word-in-topic-distribution: } \phi_k \sim Dir(\beta) \\
&\textit{Sample word-topic from topic-distribution: } z_{mn} \sim Cat(\theta_m) \\
&\textit{Sample word from word-in-topic-distribution: } w_{mn} \sim Cat(\phi[z_{mn}])
\end{align*}
\normalsize
Where $\alpha \in \R^K, \beta \in \R^{|V|}$ are the parameters for the dirichlet distribution. 

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/lda_plate}
	\caption{Plate notation for LDA with Dirichlet distributed topic-word distributions.}
	\label{fig:ldaplate}
\end{figure}


\subsubsection{Extended LDA}
So naturally I thought it should be simple to extend LDA so that it can also predict hash tags in the following way. For some document and its document-topic distribution I sample 2 topics and then for the topic-word distribution I sample a word and additionally to the topic-word distribution I just have an topic-hashtag distribution of which I sample a hashtag. And the we can learn the correct distributions since we observe both word and hashtag. \textbf{More formally} we have only 3 additions (inserted in red) to the process as defined in normal lda: 

\tiny
\begin{align*}
&\textit{Given } \alpha, \beta, \gamma \\
&\textit{Sample topic-distribution: } \theta_m \sim Dir(\alpha)\\
&\textit{Sample word-in-topic-distribution: } \phi_k \sim Dir(\beta) \\
& \textcolor{blue}{\textit{Sample hashtag-in-topic-distribution: }} \kappa_k \sim Dir(\gamma) \\
&\textit{Sample word-topic from topic-distribution: } z^{(w)}_{mn} \sim Cat(\theta_m) \\
&\textit{Sample word from word-in-topic-distribution: } w_{mn} \sim Cat(\phi[z_{mn}])\\
&\textcolor{blue}{\textit{Hashtag-topic from topic-distribution: }} z^{(t)}_{mt} \sim Cat(\theta_m) \\
&\textcolor{blue}{\textit{Hashtag from hashtag-in-topic-distribution: }} t_{mn} \sim Cat(\kappa[z_{mn}])\\
\end{align*}
\normalsize
Where $\alpha \in \R^K, \beta \in \R^{|V|}, \gamma \in \R^K$ are the parameters for the dirichlet distribution. The plate graphics in Figure \ref{fig:ldaplate2} depicts the whole process.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/extended_lda}
	\caption{Plate diagram of extended version of LDA with additional distribution for hashtags for a given topic. }
	\label{fig:ldaplate2}
\end{figure}

\subsection{Training}
Now the generative model is only the first step because we're not interested in generating words. We're interested in the so called posteriori distribution: Given words what are the topic-distribution of a document and then from that generate hashtags for the document. This means that we have a so called inference problem and normally one applies bayes formula to get the topic distribution: \[ p(\theta_m | w) = \frac{p(w | \theta_m)p(\theta_m)}{p(w)} \] But the so called evidence term $p(w) = \int_{\theta_m} p(w | \theta_m)p(\theta_m) $ is very hard to compute because every new possible topic increases computing time exponentially so this is nearly impossible to compute already for very little amount of topics. 
Researchers have proposed different methods to approximate this posteriori distribution with other methods and there are a lot of software frameworks that automatically calculate the posterior distirbution. I decided to use pyro \cite{bingham2018pyro} which allows to describe the generative model in a very pytonian way and uses pyTorch \cite{paszke2017automatic} as backend for matrix and optimization calculations which makes it ideal for big datasets. \todo{rename https://github.com/SimiPro/robotjudge/blob/master/Untitled.ipynb and cite as pyro implementation}. The implementation can be seen in \todo{cite my pyro impl. }.   The optimization process is either via a monte carlo method (MCMC) or via Variational Inference (VI). Bot methods are explained in the next two subsections very briefly. 

\subsubsection{Variational Inference}
The big idea in Variational Inference (VI) is to propose a proposal distribution $q_\lambda(\theta_m)$ which gets adapted via parameter $\lambda$ to be as close as possible to the real distribution we care about $p(\theta_m | w)$. Closeness is measured via kullback-leibler divergence but this can't be calculated directly most of the time so alternatively one maximizes the so called ELBO which minimizes the kullback-leibler divergence. 
\[ ELBO = \mathbb{E}_{q_\lambda(\theta_m)} [log p(\theta_m, w) - log q_{\lambda}(\theta_m)] \]
Then to maximize the ELBO in most software packages one calculates the gradient and applies one of the nowaways many optimization algorithms. (Adam, RMSprop, SGD, etc.)\\
Since I used pyro to implement this, in pyro $q_\lambda$ is described in the so called "guide" function and the joint distribution $p(\theta_m, w)$ is described in the so called "model". 

\subsubsection{Markov Chain Monte Carlo}
Markov Chain Monte Carlo (MCMC) algorithms are a bit more involved in theory and there are a lot more flavors then in variational inference. But the underlying idea is the same for them aswell. MCMC algorithms propose a markov chain from which they draw correlated samples when moving from state to state in this chain but which promises to converge to the correct distribution. Now one may ask why we do not use this all the time when it converges to the correct distribution ? Sadly such a chain has a so called "warm up" stage where we have to throw away samples up to the point where the distribution is burned it and we do not know how long this warm up phase is so most of the time one tests this empirically.\\



\subsubsection{Training Result}

But it just didn't work. I tried a multitude of different proposal distributions and even neural networks. It just didn't really converge nicely. I initially thought that the problem was my extension on top of LDA. But it didn't work for LDA aswell. 


\subsection{Tweet2vec - Recurrent Neural Network}
After the extended LDA somewhat failed I tried a multitude of other models. The one that finally worked was a neural network based solution: Tweet2Vec \cite{DhingraZFMC16}. The basic idea behind this model is that they view the character as smallest unit rather then the word as in LDA. This brings quite a few advantages: \todo{How many unique words are in the twitter dataset ? Which makes it difficult to use word based systems. }. Furthermore they assume that a tweet has some latent representation and from this latent representation they predict the hashtag. To get a loss they use cross entropy loss based on predicted hashtag versus given hashtag. The neural network is mainly built out of Bi-directional Gated Recurrent Unit (Bi-GRU) which behave similarly as LSTM's (Long short-term memory) blocks. The size of the input is reasonable constraint on the number of characters a tweet can have - 145. The embedding part of the tweet2vec neural network can be seen in Figure \ref{fig:tweet2vecembedding}. The decoder part is just a dense layer with the number of neurons equal to the number of hashtags. The output of this dense layer is then used as input for the softmax function to get the probability of each hashtag. 
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/tweet2vec_embedding}
	\caption{Tweet2vec embedding part of the neural network.}
	\label{fig:tweet2vecembedding}
\end{figure}


\begin{table}[]
	\centering
	\begin{tabular}{@{}lll@{}}
		\toprule
		Name       & Party      & Num. Speeches \\ \midrule
		Feinstein  & D   &       2        \\
		Stabenow   & D   &       1        \\
		Klobuchar  & D   &     1          \\
		Menendez   & D   &     1          \\
		Cantwell   & D   &      1         \\
		Warren     & D   &   1            \\
		Gillibrand & D   &        1       \\
		Tester     & D   &     1          \\
		Manchin    & D   &      1         \\
		Murphy     & D   &     5          \\
		Baldwin    & D   &       1        \\
		Hirono     & D   &        1       \\
		Carper     & D   &      1         \\
		Sanders    & I   &      1         \\
		Kaine      & D   &         1      \\
		Brown      & D   &      3         \\
		Heinrich   & D   &       1        \\
		Whitehouse & D   &     1          \\
		Casey      & D   &     1          \\
		Wicker     & R &         0      \\
		Barrasso   & R &    2           \\
		Fischer    & R &         1      \\
		Cruz       & R &      2         \\
		King       & I &     3          \\ \bottomrule
	\end{tabular}
	\caption{List of senators up for reelection in 2018. (D = Democrat, R = Republican, I = Independent)}
	\label{tbl:senators}
\end{table}


\section{Experiments \& Results}
\label{sec:Experiments}
First lets reiterate more concrete about what we want to figure out and then subsequently how we design the experiments to test our hypothesis. 
We start with a simple one: Can we tag speeches of politicians with hashtags learned from twitter ? Then we go on to figure out how tags compare for someone giving a speech before and after campaigning for reelection. 

\subsection{Can we tag speeches of politicians with hashtags learned from twitter ? }
\subsubsection{Data}
As explained in previous sections I used tweet2vec. The politician set consisted of 449334 tweets which we splitted randomly into 90\% training data and 10\% testing data. I counted all the hashtags and removed all that were not in the top 500 used hashtags (from 39671 total).  In Figure \ref{fig:tweet2vectraining} one can see that we got nearly 25\% recall for tag number 1 which is quite good considering how difficult the task is (Randomly predicting one of the top 500 would result in 0.2\% recall). For the speeches I only was interested in the senators up for re-election. It is a set of 24 senators listed in table with their corresponding party and the number of speeches they gave in the years 2017 and 2018. \ref{tbl:senators}. \\
It becomes very obvious that the dataset of speeches is very small. But it does not hinder us to tag the available speeches. To tweet2vec to be able to work on the speeches I had to cut it up into pieces of 145 and additionally I did some preprocessing like stemming, removing stop words and removing line breaks just to get a bit shorter speeches and get more content into those 145 characters. 


\subsubsection{Results}

In the boxes one can see the input for the trained tweet2vec aswell as the tags tweet2vec predicted. 

\begin{framed}
	\textbf{Senator Sanders}: \\
	support vote pension issue million american worker percent pension promise government allow happen stand worker meeting earlier today dealing \\
	Tags: Kavanaugh WhatAreTheyHiding NM UK IIOA
\end{framed}
\begin{framed}
	\textbf{Senator Cruz}: \\
 president authority refuse enforce president obama decree would enforce federal immigration exactly virtually every republican denounce executiv
 \\
	Tags: ACA healthcare NM ClimateAction ProtectOurCare
\end{framed}

I really tried to also find a positive example but I just couldn't really find one. From just checking the speeches vs the tags it does not seem to be very effective in predicting tags for speeches. 

 
\subsection{How does being before an election affect the tags a politican uses ? }
After wasting a lot of time on the tagging speeches part I concentrated the little rest of my time on only analyzing the raw twitter data. And one can say that for some senators definitely tweets differently and is affected when having an election. I picked a democratic senator and a republican senator to showcase how tweets change. Local topic get very important for senators and most tweets will get tagged with the state their from. In figure \ref{fig:hironobeforeafterelection} its the case for the 2012 election where Senator Hirono wants to get elected for Hawaii. In \ref{fig:senatorfischerbeforeafterelection} one can see the figure of number of tweets at a specific date from Senator Fisher of the republicans. Shortly before the election Nebraska specific tweets definitely spiked. But it is definitely not en if and only if indicator.  There are a lot of Senators who don't care about tweeting for local problems e.g. Senator Warren and Senator Sanders. The plots for all the other Senators can be checked in my github repository. \todo{make a link to the github repo in the introduction}. But not only could it be that the tweet behaviour does not change but that a Senator could start tweeting about local events with no local election coming up. An example for this is Senator Cruz who had a hard election in Texas. But in the year of the election there was also a hurrican as strong as all 10 years and he tweeted a lot about that in that time frame seen in Figure \ref{fig:senatorcruztop5tweetsmid20172018}.
 
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/hirono_before_after_election}
	\caption{Cumulative sum of top 5 topics of senator Hirono from Hawaii. }
	\label{fig:hironobeforeafterelection}
\end{figure}


\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/senator_fischer_before_after_election}
	\caption{Senator Fischer from Nebraska number of tweets from 2012 - 2019 counted at specific points in time. Only tweets counted with tags of top 8 tags. \#ne = nebraska, \#neag = nebraska agrar, \#lovene = love nebraska.   }
	\label{fig:senatorfischerbeforeafterelection}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/senator_cruz_top5_tweets_mid2017_2018}
	\caption{Senator Cruz from Texas top 5 used tags tweets distribution from mid 2017 to end 2018.}
	\label{fig:senatorcruztop5tweetsmid20172018}
\end{figure}


\subsection{Hashtag analysis}


\subsection{Tweet2vec}


\subsection{Tweet2Vec}
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/tweet2vec_training}
	\caption{Results for testing on validation set. (10\% of original dataset).}
	\label{fig:tweet2vectraining}
\end{figure}

\section{Negative Results}


\section{Discussion}
\label{sec:Discussion}

\todo{}

\todo{- just very few speeches data. }

\section{Summary}
\label{sec:Summary}



\bibliographystyle{IEEEtran}
\bibliography{robot_writeup}
\end{document}
